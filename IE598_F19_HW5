import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from pandas import DataFrame
import numpy as np
from sklearn.decomposition import PCA
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import math
from sklearn.svm import SVC

df = pd.read_csv('D:\hw5_treasury yield curve data.csv')

df1 = df.drop(columns = 'Date')
print(df1.describe())

sns.set()
plt.hist(df1['Adj_Close'])
plt.xlabel('Adjust close price')
plt.ylabel('count')
plt.show()

corMat = DataFrame(df2.corr())
plt.pcolor(corMat)
plt.show()

cols = ['SVENF01', 'SVENF02', 'SVENF03', 'SVENF04', 'SVENF05', 'Adj_Close']
cm = np.corrcoef(df2[cols].values.T)
sns.set(font_scale = 1.5)
hm = sns.heatmap(cm, cbar = True, annot = True, square = True, fmt = '.2f', annot_kws = {'size': 15}, yticklabels = cols, xticklabels = cols)
plt.show()

plt.plot(df2["SVENF07"], df2["Adj_Close"], marker = '.', linestyle='none')
plt.xlabel('SVENF07')
plt.ylabel('Adjusted Close Price')
plt.show()

#df2 = pd.DataFrame(df1).fillna(df1.mean())
df2 = df1.replace(0, np.NaN)
# drop rows with missing values
df2.dropna(inplace=True)
X, y = df2.drop(columns = ['Adj_Close']).values, df2['Adj_Close'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
print('\nEigenvalues\n%s' % eigen_vals)

tot = sum(eigen_vals)
var_exp = [(i / tot) for i in sorted(eigen_vals, reverse = True)]
cum_var_exp = np.cumsum(var_exp)
print("explained variance ratio: ", var_exp)
plt.bar(range(1, 31), var_exp, alpha = 0.5, align = 'center', label = 'individual explained variance')
plt.step(range(1, 31), cum_var_exp, where = 'mid', label = 'cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Princial component index')
plt.legend(loc = 'best')
plt.show()

eigen_vals2 = sorted(eigen_vals, reverse = True)[:3]
tot = sum(eigen_vals)
var_exp = [(i / tot) for i in eigen_vals2]
cum_var_exp = np.cumsum(var_exp)
print("explained variance ratio: ", var_exp)
print("cumulative explained variance ratio: ", cum_var_exp)
plt.bar(range(1, 4), var_exp, alpha = 0.5, align = 'center', label = 'individual explained variance')
plt.step(range(1, 4), cum_var_exp, where = 'mid', label = 'cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Princial component index')
plt.legend(loc = 'best')
plt.show()

X, y = df2.drop(columns = ['Adj_Close']).values, df2['Adj_Close'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)

reg_all = LinearRegression()
start = time.time()
reg_all.fit(X_train, y_train)
y_train_pred = reg_all.predict(X_train)
y_pred = reg_all.predict(X_test)
end = time.time()
print("Linear Regression for all attributes:")
print('RMSE:  Train: %.3f, Test: %.3f' % ((np.sqrt(mean_squared_error(y_train, y_train_pred))),np.sqrt(mean_squared_error(y_test, y_pred))))
print('R^2 Train: %.3f, Test: %.3f' %(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))
print('time: %.5f s' % (end - start))

pca = PCA(n_components = 3)
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)
start = time.time()
reg_all.fit(X_train_pca, y_train)
y_train_pred = reg_all.predict(X_train_pca)
y_pred = reg_all.predict(X_test_pca)
end = time.time()
print("Linear Regression for 3 components:")
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_pred)
print('RMSE Train: %.3f, Test: %.3f' %(math.sqrt(mse_train), math.sqrt(mse_test)))
print('R^2 Train: %.3f, Test: %.3f' %(r2_score(y_train, y_train_pred), r2_score(y_test, y_pred)))
print('time: %.5f s' % (end - start))

svm = SVR(kernel="linear")
start = time.time()
svm.fit(X_train, y_train)
y_train_pred = svm.predict(X_train)
y_pred = svm.predict(X_test)
end = time.time()
print("SVM for all attributes:")
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_pred)
print('RMSE Train: %.3f, Test: %.3f' %(math.sqrt(mse_train), math.sqrt(mse_test)))
print('R^2 Train: %.3f, Test: %.3f' %(r2_score(y_train, y_train_pred), r2_score(y_test, y_pred)))
print('time: %.5f s' % (end - start))

pca = PCA(n_components = 3)
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)
start = time.time()
svm.fit(X_train_pca, y_train)
y_train_pred = svm.predict(X_train_pca)
y_pred = svm.predict(X_test_pca)
end = time.time()
print("SVM for 3 components:")
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_pred)
print('RMSE Train: %.3f, Test: %.3f' %(math.sqrt(mse_train), math.sqrt(mse_test)))
print('R^2 Train: %.3f, Test: %.3f' %(r2_score(y_train, y_train_pred), r2_score(y_test, y_pred)))
print('time: %.5f s' % (end - start))

print('My name is Xinya Liu')
print('My NetID is xinyal2')
print('I hereby certify that I have read the University policy on Academic Integrity and that I am not in violation.')
